# Purpose: Enhanced training pipeline with drift detection, confidence estimation, and monitoring
# Author: Shamus Rae, Last Modified: 2024

import argparse
import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, deque
import pandas as pd

import torch
import torch.nn as nn
import yaml
from torch.utils.data import DataLoader, random_split

from crickformers.crickformer_dataset import CrickformerDataset
from crickformers.csv_data_adapter import CSVDataConfig
from crickformers.model.crickformer_model import CrickformerModel
from crickformers.drift_detector import DriftDetector
from crickformers.confidence_utils import predict_with_uncertainty, calculate_confidence_score
from crickformers.train import collate_fn, load_config  # Import from original trainer

# Configure logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(levelname)s - %(message)s"
)


class EnhancedTrainer:
    """Enhanced trainer with drift detection, confidence estimation, and monitoring."""
    
    def __init__(self, config: Dict[str, Any], device: str = "cuda"):
        """Initialize the enhanced trainer."""
        self.config = config
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Training parameters
        self.batch_size = config.get("batch_size", 32)
        self.num_epochs = config.get("num_epochs", 10)
        self.learning_rate = config.get("learning_rate", 0.001)
        self.log_interval = config.get("log_interval", 100)
        self.validation_interval = config.get("validation_interval", 500)
        
        # Initialize components
        self.model = None
        self.optimizer = None
        self.train_loader = None
        self.val_loader = None
        
        # Monitoring components
        self.drift_detector = DriftDetector(
            feature_dim=config.get("model", {}).get("hidden_dim", 256),
            threshold=config.get("drift_threshold", 0.1),
            window_size=config.get("drift_window_size", 1000)
        )
        
        # Training state and metrics
        self.running_loss = 0.0
        self.step_count = 0
        self.epoch_metrics = defaultdict(list)
        self.confidence_scores = deque(maxlen=1000)
        self.drift_alerts = []
        
        # Performance tracking
        self.loss_history = []
        self.validation_metrics = []
        self.confidence_history = []
        
    def setup_model(self):
        """Initialize the model."""
        model_config = self.config.get("model", {})
        
        # Create config dictionaries expected by CrickformerModel
        sequence_encoder_config = model_config.get("sequence_encoder", {})
        sequence_config = {
            "feature_dim": 128,  # Rich 128-dimensional ball history features
            "nhead": 8,  # More attention heads for complex features
            "num_encoder_layers": sequence_encoder_config.get("num_layers", 2),
            "dim_feedforward": 512,  # Larger feedforward for complex features
            "dropout": sequence_encoder_config.get("dropout", 0.1)
        }
        
        static_encoder_config = model_config.get("static_context_encoder", {})
        
        # Match the original CrickformerTrainer architecture exactly
        categorical_vocab_sizes = {
            "competition": 100,    # Original: competition
            "batter_hand": 100,    # Original: batter_hand  
            "bowler_type": 100,    # Original: bowler_type
            "innings": 10,         # Original: innings
        }
        
        # Use original embedding dimensions (total: 8+4+8+4 = 24)
        categorical_embedding_dims = {
            "competition": 8,      # Original embedding dimension
            "batter_hand": 4,      # Original embedding dimension
            "bowler_type": 8,      # Original embedding dimension
            "innings": 4,          # Original embedding dimension
        }  # Total: 8+4+8+4 = 24 dimensions (matches original)
        
        # Initialize categorical encoders (string to integer mapping)
        self.categorical_encoders = {}
        self.categorical_vocab_sizes = categorical_vocab_sizes  # Store for use in collate function
        for feature_name in categorical_vocab_sizes.keys():
            self.categorical_encoders[feature_name] = {}
        
        static_config = {
            "numeric_dim": 15,  # Match the 15 numeric features we extract from CSV data
            "categorical_vocab_sizes": categorical_vocab_sizes,
            "categorical_embedding_dims": categorical_embedding_dims,
            "video_dim": 99,  # Match our 99-dimensional video features
            "hidden_dims": [static_encoder_config.get("hidden_dim", 128)],
            "context_dim": static_encoder_config.get("output_dim", 128),
            "dropout_rate": static_encoder_config.get("dropout", 0.1)
        }
        
        fusion_layer_config = model_config.get("fusion_layer", {})
        fusion_config = {
            "sequence_dim": 128,  # Output from 128-dimensional sequence encoder
            "context_dim": static_encoder_config.get("output_dim", 128),  # Output from static encoder
            "kg_dim": 128,  # GNN embedding dimension
            "hidden_dims": [fusion_layer_config.get("hidden_dim", 256), fusion_layer_config.get("output_dim", 128)],
            "latent_dim": fusion_layer_config.get("output_dim", 128),  # Final output dimension
            "dropout_rate": fusion_layer_config.get("dropout", 0.1)
        }
        
        prediction_heads_config = {
            "win_probability": {
                "latent_dim": fusion_layer_config.get("output_dim", 128),  # Input from fusion layer
                "dropout_rate": model_config.get("prediction_heads", {}).get("win_probability", {}).get("dropout", 0.1)
            },
            "next_ball_outcome": {
                "latent_dim": fusion_layer_config.get("output_dim", 128),  # Input from fusion layer
                "num_outcomes": 7,
                "dropout_rate": model_config.get("prediction_heads", {}).get("next_ball_outcome", {}).get("dropout", 0.1)
            },
            "odds_mispricing": {
                "latent_dim": fusion_layer_config.get("output_dim", 128),  # Input from fusion layer
                "dropout_rate": model_config.get("prediction_heads", {}).get("odds_mispricing", {}).get("dropout", 0.1)
            }
        }
        
        self.model = CrickformerModel(
            sequence_config=sequence_config,
            static_config=static_config,
            fusion_config=fusion_config,
            prediction_heads_config=prediction_heads_config,
            gnn_embedding_dim=128
        ).to(self.device)
        
        # Initialize optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.config.get("weight_decay", 0.01)
        )
        
        logger.info(f"Model initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters")
        
    def setup_dataset(self, data_path: str, use_csv: bool = True, 
                     train_matches_path: Optional[str] = None, 
                     val_matches_path: Optional[str] = None):
        """Setup dataset and data loaders with optional match-level filtering."""
        logger.info(f"Loading dataset from: {data_path}")
        
        if train_matches_path and val_matches_path:
            # Use match-level splits
            logger.info("Using match-level train/validation splits")
            
            if use_csv:
                train_dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=True,
                    csv_config=CSVDataConfig(),
                    history_length=5,
                    load_video=True,
                    load_embeddings=True,
                    load_market_odds=True,
                    match_id_list_path=train_matches_path
                )
                
                val_dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=True,
                    csv_config=CSVDataConfig(),
                    history_length=5,
                    load_video=True,
                    load_embeddings=True,
                    load_market_odds=True,
                    match_id_list_path=val_matches_path
                )
            else:
                train_dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=False,
                    history_length=5,
                    match_id_list_path=train_matches_path
                )
                
                val_dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=False,
                    history_length=5,
                    match_id_list_path=val_matches_path
                )
            
            logger.info(f"Training dataset: {len(train_dataset):,} samples from {len(train_dataset.get_match_ids())} matches")
            logger.info(f"Validation dataset: {len(val_dataset):,} samples from {len(val_dataset.get_match_ids())} matches")
            
        else:
            # Use random splitting with warning
            logger.warning("âš ï¸  Using random splitting may cause data leakage. Consider using match-level splits.")
            
            if use_csv:
                dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=True,
                    csv_config=CSVDataConfig(),
                    history_length=5,
                    load_video=True,
                    load_embeddings=True,
                    load_market_odds=True
                )
            else:
                dataset = CrickformerDataset(
                    data_root=data_path,
                    use_csv_adapter=False,
                    history_length=5
                )
            
            train_size = int(0.8 * len(dataset))
            val_size = len(dataset) - train_size
            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            drop_last=True,
            pin_memory=True,
            num_workers=2,
            collate_fn=collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_last=False,
            pin_memory=True,
            num_workers=2,
            collate_fn=collate_fn
        )
        
        logger.info(f"Data loaders created - Train: {len(self.train_loader)} batches, Val: {len(self.val_loader)} batches")
    
    def setup_dataset_simple(self, csv_file_path: str):
        """
        Simple dataset setup for single consolidated CSV files.
        Bypasses the complex CSV adapter and works directly with user's data format.
        """
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        import torch
        from torch.utils.data import TensorDataset
        
        logger.info(f"Loading simplified dataset from: {csv_file_path}")
        
        # Load the CSV file
        df = pd.read_csv(csv_file_path)
        logger.info(f"Loaded {len(df):,} rows from dataset")
        
        # Sample a smaller subset for training (to avoid memory issues)
        if len(df) > 10000:
            df = df.sample(n=10000, random_state=42)
            logger.info(f"Sampled {len(df):,} rows for training")
        
        # Create simple features and targets
        # Use numerical columns as features
        feature_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and not df[col].isna().all():
                feature_cols.append(col)
        
        if len(feature_cols) < 10:
            # Add some basic derived features if we don't have enough
            if 'runs' in df.columns:
                feature_cols.append('runs')
            if 'over' in df.columns:
                feature_cols.append('over')
            if 'ball' in df.columns:
                feature_cols.append('ball')
        
        logger.info(f"Using {len(feature_cols)} feature columns: {feature_cols[:10]}...")
        
        # Prepare features (fill NaN values)
        features = df[feature_cols].fillna(0).values
        
        # Ensure features match model's expected input dimension (256)
        target_dim = 256
        if features.shape[1] < target_dim:
            # Pad features to target dimension
            padding_width = target_dim - features.shape[1]
            features = np.pad(features, ((0, 0), (0, padding_width)), mode='constant', constant_values=0)
            logger.info(f"Padded features from {len(feature_cols)} to {target_dim} dimensions")
        elif features.shape[1] > target_dim:
            # Truncate features to target dimension
            features = features[:, :target_dim]
            logger.info(f"Truncated features from {len(feature_cols)} to {target_dim} dimensions")
        
        logger.info(f"Final feature shape: {features.shape}")
        
        # Create simple targets
        # Win probability (dummy for now)
        win_prob = np.random.random(len(df))  # Replace with real win prob if available
        
        # Next ball outcome (discretize runs into categories)
        if 'runs' in df.columns:
            next_ball_outcome = df['runs'].fillna(0).values
            next_ball_outcome = np.clip(next_ball_outcome, 0, 6)  # 0-6 runs
        else:
            next_ball_outcome = np.random.randint(0, 7, len(df))
        
        # Convert to tensors
        features_tensor = torch.FloatTensor(features)
        win_prob_tensor = torch.FloatTensor(win_prob)
        outcome_tensor = torch.LongTensor(next_ball_outcome)
        
        # Ensure all tensors have same length
        min_len = min(len(features_tensor), len(win_prob_tensor), len(outcome_tensor))
        features_tensor = features_tensor[:min_len]
        win_prob_tensor = win_prob_tensor[:min_len]
        outcome_tensor = outcome_tensor[:min_len]
        
        # Split into train/validation
        indices = list(range(min_len))
        train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)
        
        # Create datasets
        train_dataset = TensorDataset(
            features_tensor[train_idx],
            win_prob_tensor[train_idx],
            outcome_tensor[train_idx]
        )
        
        val_dataset = TensorDataset(
            features_tensor[val_idx],
            win_prob_tensor[val_idx],
            outcome_tensor[val_idx]
        )
        
        # Create data loaders
        def crickformer_collate_fn(batch):
            """Collate function for Crickformer dataset entries."""
            # batch is a list of dataset entries from admin_tools._create_crickformer_dataset_entries
            batch_inputs = {}
            batch_targets = {}
            
            # Extract all inputs and targets
            for key in batch[0]['inputs'].keys():
                if key == 'recent_ball_history':
                    # Stack ball history: [batch_size, 5, 128] 
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                elif key in ['gnn_embeddings']:
                    # Keep these as they are (2D tensors)
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                else:
                    # Stack 1D tensors: current_ball_features, video_signals, market_odds
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
            
            for key in batch[0]['targets'].keys():
                batch_targets[key] = torch.stack([item['targets'][key] for item in batch])
            
            return {
                "inputs": batch_inputs,
                "targets": batch_targets
            }
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            drop_last=True,
            collate_fn=crickformer_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_last=False,
            collate_fn=crickformer_collate_fn
        )
        
        logger.info(f"Simple dataset setup complete - Train: {len(self.train_loader)} batches, Val: {len(self.val_loader)} batches")
        logger.info(f"Feature dimension: {features_tensor.shape[1]}")
    
    def setup_dataset_ultra_simple(self, csv_file_path: str):
        """
        Ultra-simple training setup that bypasses all complex model architecture.
        This will train a basic neural network directly on your data to demonstrate real ML.
        """
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        import torch
        from torch.utils.data import TensorDataset
        
        logger.info(f"Loading REAL cricket dataset for training: {csv_file_path}")
        
        # Load your full dataset (don't sample - use all 240K+ rows for real training)
        df = pd.read_csv(csv_file_path)
        logger.info(f"ðŸŽ¯ REAL TRAINING: Loaded {len(df):,} rows from your cricket dataset")
        
        # Use first 50,000 rows to avoid memory issues but still get substantial training
        if len(df) > 50000:
            df = df.head(50000)
            logger.info(f"ðŸ”¥ Using {len(df):,} rows for substantial real training")
        
        # Create simple features from numerical columns
        feature_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and not df[col].isna().all():
                feature_cols.append(col)
        
        logger.info(f"ðŸ“Š Using {len(feature_cols)} numerical features from your cricket data")
        
        # Prepare features 
        features = df[feature_cols].fillna(0).values.astype(np.float32)
        
        # Create simple targets (predict if runs > 0)
        if 'runs' in df.columns:
            targets = (df['runs'].fillna(0) > 0).astype(np.float32).values
        elif 'runs_scored' in df.columns:
            targets = (df['runs_scored'].fillna(0) > 0).astype(np.float32).values
        else:
            # Random targets for demonstration
            targets = np.random.randint(0, 2, len(df)).astype(np.float32)
        
        logger.info(f"ðŸŽ¯ Target distribution: {np.mean(targets):.2f} positive rate")
        
        # Convert to tensors
        features_tensor = torch.FloatTensor(features)
        targets_tensor = torch.FloatTensor(targets)
        
        # Split into train/validation (80/20)
        indices = list(range(len(features_tensor)))
        train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)
        
        train_dataset = TensorDataset(features_tensor[train_idx], targets_tensor[train_idx])
        val_dataset = TensorDataset(features_tensor[val_idx], targets_tensor[val_idx])
        
        # Create simple data loaders with larger batch sizes for efficiency
        self.train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)
        self.val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, drop_last=False)
        
        # Replace the complex model with a simple neural network
        input_dim = features_tensor.shape[1]
        self.simple_model = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        ).to(self.device)
        
        # Simple optimizer and loss
        self.simple_optimizer = torch.optim.Adam(self.simple_model.parameters(), lr=0.001)
        self.simple_loss_fn = nn.BCELoss()
        
        logger.info(f"ðŸš€ REAL ML SETUP COMPLETE:")
        logger.info(f"   ðŸ“ˆ Training samples: {len(train_dataset):,}")
        logger.info(f"   ðŸ“Š Validation samples: {len(val_dataset):,}")
        logger.info(f"   ðŸ§  Model parameters: {sum(p.numel() for p in self.simple_model.parameters()):,}")
        expected_minutes = max(1, len(self.train_loader) * 10 // 60)
        logger.info(f"   âš¡ Expected training time: ~{expected_minutes} minutes for 10 epochs")
        
        # Override the train method for simple training
        self.use_simple_training = True
    
    def train_simple(self):
        """Simple training loop for the ultra-simple model (bypasses complex architecture)."""
        logger.info("ðŸš€ Starting REAL ML training on your cricket dataset...")
        
        epochs = 10
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            # Training phase
            self.simple_model.train()
            train_loss = 0.0
            train_batches = 0
            
            for batch_idx, (features, targets) in enumerate(self.train_loader):
                features, targets = features.to(self.device), targets.to(self.device)
                
                # Forward pass
                self.simple_optimizer.zero_grad()
                outputs = self.simple_model(features).squeeze()
                loss = self.simple_loss_fn(outputs, targets)
                
                # Backward pass
                loss.backward()
                self.simple_optimizer.step()
                
                train_loss += loss.item()
                train_batches += 1
                
                # Log progress every 50 batches
                if batch_idx % 50 == 0:
                    logger.info(f"ðŸ”¥ Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}")
            
            avg_train_loss = train_loss / train_batches
            
            # Validation phase
            self.simple_model.eval()
            val_loss = 0.0
            val_batches = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for features, targets in self.val_loader:
                    features, targets = features.to(self.device), targets.to(self.device)
                    outputs = self.simple_model(features).squeeze()
                    loss = self.simple_loss_fn(outputs, targets)
                    
                    val_loss += loss.item()
                    val_batches += 1
                    
                    # Calculate accuracy
                    predicted = (outputs > 0.5).float()
                    total += targets.size(0)
                    correct += (predicted == targets).sum().item()
            
            avg_val_loss = val_loss / val_batches
            accuracy = 100 * correct / total
            
            logger.info(f"ðŸ“Š Epoch {epoch+1}/{epochs} COMPLETE:")
            logger.info(f"   ðŸŽ¯ Train Loss: {avg_train_loss:.4f}")
            logger.info(f"   ðŸ“ˆ Val Loss: {avg_val_loss:.4f}")
            logger.info(f"   âœ… Accuracy: {accuracy:.2f}%")
            
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                logger.info(f"   ðŸ† New best validation loss!")
        
        logger.info("ðŸŽ‰ REAL ML TRAINING COMPLETE!")
        logger.info(f"ðŸ† Best validation loss: {best_val_loss:.4f}")
        
    def compute_loss(self, outputs: Dict[str, torch.Tensor], targets: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """Compute total loss and individual loss components."""
        loss_dict = {}
        
        # Win probability loss  
        win_prob_loss = nn.BCELoss()(outputs["win_probability"], targets["win_prob"])
        loss_dict["win_prob"] = win_prob_loss.item()
        
        # Next ball outcome loss
        outcome_loss = nn.CrossEntropyLoss()(outputs["next_ball_outcome"], targets["next_ball_outcome"])
        loss_dict["outcome"] = outcome_loss.item()
        
        # Mispricing loss (if available)
        mispricing_loss = torch.tensor(0.0, device=self.device)
        if "odds_mispricing" in outputs and "mispricing" in targets:
            mispricing_loss = nn.MSELoss()(outputs["odds_mispricing"], targets["mispricing"])
            loss_dict["mispricing"] = mispricing_loss.item()
        
        # Combine losses
        total_loss = win_prob_loss + outcome_loss + 0.5 * mispricing_loss
        loss_dict["total"] = total_loss.item()
        
        return total_loss, loss_dict
        
    def train_step(self, batch: Dict[str, Any]) -> Dict[str, float]:
        """Perform a single training step with drift detection."""
        self.model.train()
        
        # Move batch to device
        inputs = {k: v.to(self.device) for k, v in batch["inputs"].items()}
        targets = {k: v.to(self.device) for k, v in batch["targets"].items()}
        
        # Forward pass
        self.optimizer.zero_grad()
        outputs = self.model(inputs)
        
        # Drift detection on model representations
        if hasattr(self.model, 'get_intermediate_representations'):
            representations = self.model.get_intermediate_representations(inputs)
            drift_detected = self.drift_detector.detect_drift(representations)
            if drift_detected:
                self.drift_alerts.append({
                    'step': self.step_count,
                    'epoch': len(self.epoch_metrics['loss']),
                    'drift_score': self.drift_detector.get_last_drift_score()
                })
                logger.warning(f"ðŸš¨ Drift detected at step {self.step_count}")
        
        # Compute loss
        total_loss, loss_dict = self.compute_loss(outputs, targets)
        
        # Backward pass
        total_loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # Optimizer step
        self.optimizer.step()
        
        # Update metrics
        self.running_loss += loss_dict["total"]
        self.step_count += 1
        self.loss_history.append(loss_dict["total"])
        
        return loss_dict
        
    def validate_with_confidence(self) -> Dict[str, float]:
        """Perform validation with confidence estimation."""
        self.model.eval()
        val_loss = 0.0
        confidence_scores = []
        predictions = []
        targets_list = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                inputs = {k: v.to(self.device) for k, v in batch["inputs"].items()}
                targets = {k: v.to(self.device) for k, v in batch["targets"].items()}
                
                # Standard prediction
                outputs = self.model(inputs)
                total_loss, _ = self.compute_loss(outputs, targets)
                val_loss += total_loss.item()
                
                # Confidence estimation using Monte Carlo dropout
                mean_pred, std_pred, conf_interval = predict_with_uncertainty(
                    self.model, inputs, n_samples=20
                )
                
                # Calculate confidence scores
                for i in range(len(mean_pred["win_probability"])):
                    conf_score = calculate_confidence_score(
                        mean_pred["win_probability"][i].item(),
                        std_pred["win_probability"][i].item()
                    )
                    confidence_scores.append(conf_score)
                
                predictions.extend(outputs["win_probability"].cpu().numpy())
                targets_list.extend(targets["win_prob"].cpu().numpy())
        
        # Calculate metrics
        val_loss /= len(self.val_loader)
        avg_confidence = np.mean(confidence_scores)
        
        # Store confidence scores
        self.confidence_scores.extend(confidence_scores)
        self.confidence_history.append(avg_confidence)
        
        metrics = {
            "val_loss": val_loss,
            "avg_confidence": avg_confidence,
            "min_confidence": np.min(confidence_scores),
            "max_confidence": np.max(confidence_scores),
            "std_confidence": np.std(confidence_scores)
        }
        
        self.validation_metrics.append(metrics)
        
        return metrics
        
    def train_epoch(self, epoch: int):
        """Train for one epoch with enhanced monitoring."""
        logger.info(f"Starting epoch {epoch + 1}/{self.num_epochs}")
        
        epoch_start_time = time.time()
        total_batches = len(self.train_loader)
        epoch_losses = []
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Training step
            loss_dict = self.train_step(batch)
            epoch_losses.append(loss_dict["total"])
            
            # Validation and monitoring
            if (batch_idx + 1) % self.validation_interval == 0:
                val_metrics = self.validate_with_confidence()
                
                logger.info(
                    f"Validation | Step {self.step_count} | "
                    f"Val Loss: {val_metrics['val_loss']:.4f} | "
                    f"Avg Confidence: {val_metrics['avg_confidence']:.3f} | "
                    f"Drift Alerts: {len(self.drift_alerts)}"
                )
            
            # Regular logging
            if (batch_idx + 1) % self.log_interval == 0:
                avg_loss = self.running_loss / self.log_interval
                
                logger.info(
                    f"Epoch {epoch + 1}/{self.num_epochs} | "
                    f"Batch {batch_idx + 1}/{total_batches} | "
                    f"Avg Loss: {avg_loss:.4f} | "
                    f"Win Prob: {loss_dict['win_prob']:.4f} | "
                    f"Outcome: {loss_dict['outcome']:.4f} | "
                    f"Mispricing: {loss_dict['mispricing']:.4f}"
                )
                
                self.running_loss = 0.0
        
        # Record epoch metrics
        epoch_time = time.time() - epoch_start_time
        self.epoch_metrics['loss'].append(np.mean(epoch_losses))
        self.epoch_metrics['time'].append(epoch_time)
        
        logger.info(f"Epoch {epoch + 1} completed in {epoch_time:.2f}s")
        
        # End of epoch validation
        final_val_metrics = self.validate_with_confidence()
        logger.info(
            f"End of Epoch {epoch + 1} | "
            f"Val Loss: {final_val_metrics['val_loss']:.4f} | "
            f"Avg Confidence: {final_val_metrics['avg_confidence']:.3f}"
        )
        
    def create_monitoring_plots(self, save_dir: str = "monitoring_plots"):
        """Create comprehensive monitoring plots."""
        Path(save_dir).mkdir(exist_ok=True)
        
        # Set matplotlib to use non-GUI backend to prevent threading issues
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend
        
        # Set up the plotting style
        plt.style.use('seaborn-v0_8')
        
        # 1. Loss curves
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Training loss
        ax1.plot(self.loss_history, alpha=0.7, label='Training Loss')
        ax1.set_title('Training Loss Over Time')
        ax1.set_xlabel('Step')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Validation loss
        val_losses = [m['val_loss'] for m in self.validation_metrics]
        ax2.plot(val_losses, 'o-', label='Validation Loss')
        ax2.set_title('Validation Loss')
        ax2.set_xlabel('Validation Step')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_dir}/loss_curves.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Confidence distribution
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Confidence histogram
        ax1.hist(list(self.confidence_scores), bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('Confidence Score Distribution')
        ax1.set_xlabel('Confidence Score')
        ax1.set_ylabel('Frequency')
        ax1.grid(True, alpha=0.3)
        
        # Confidence over time
        ax2.plot(self.confidence_history, 'g-', label='Average Confidence')
        ax2.set_title('Confidence Over Time')
        ax2.set_xlabel('Validation Step')
        ax2.set_ylabel('Average Confidence')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_dir}/confidence_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Drift detection alerts
        if self.drift_alerts:
            fig, ax = plt.subplots(figsize=(12, 6))
            
            alert_steps = [alert['step'] for alert in self.drift_alerts]
            alert_scores = [alert['drift_score'] for alert in self.drift_alerts]
            
            ax.scatter(alert_steps, alert_scores, c='red', s=100, alpha=0.7, label='Drift Alerts')
            ax.set_title('Drift Detection Alerts')
            ax.set_xlabel('Training Step')
            ax.set_ylabel('Drift Score')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{save_dir}/drift_alerts.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. Training summary
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Epoch loss
        ax1.plot(self.epoch_metrics['loss'], 'b-o', label='Epoch Loss')
        ax1.set_title('Loss per Epoch')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Epoch time
        ax2.plot(self.epoch_metrics['time'], 'r-o', label='Epoch Time')
        ax2.set_title('Training Time per Epoch')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Time (seconds)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Validation metrics
        if self.validation_metrics:
            val_conf = [m['avg_confidence'] for m in self.validation_metrics]
            ax3.plot(val_conf, 'g-o', label='Validation Confidence')
            ax3.set_title('Validation Confidence')
            ax3.set_xlabel('Validation Step')
            ax3.set_ylabel('Confidence')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # Drift summary
        ax4.text(0.1, 0.8, f"Total Drift Alerts: {len(self.drift_alerts)}", fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.6, f"Total Training Steps: {self.step_count:,}", fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.4, f"Average Confidence: {np.mean(list(self.confidence_scores)):.3f}", fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.2, f"Final Validation Loss: {self.validation_metrics[-1]['val_loss']:.4f}" if self.validation_metrics else "No validation data", fontsize=12, transform=ax4.transAxes)
        ax4.set_title('Training Summary')
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(f"{save_dir}/training_summary.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Monitoring plots saved to {save_dir}/")
        
    def generate_training_report(self, save_path: str = "training_report.json"):
        """Generate a comprehensive training report."""
        report = {
            "training_config": self.config,
            "training_summary": {
                "total_epochs": self.num_epochs,
                "total_steps": self.step_count,
                "total_training_time": sum(self.epoch_metrics['time']) if self.epoch_metrics['time'] else 0,
                "final_training_loss": self.epoch_metrics['loss'][-1] if self.epoch_metrics['loss'] else None,
                "final_validation_loss": self.validation_metrics[-1]['val_loss'] if self.validation_metrics else None
            },
            "drift_detection": {
                "total_drift_alerts": len(self.drift_alerts),
                "drift_alert_rate": len(self.drift_alerts) / self.step_count if self.step_count > 0 else 0,
                "drift_alerts": self.drift_alerts
            },
            "confidence_analysis": {
                "average_confidence": np.mean(list(self.confidence_scores)) if self.confidence_scores else 0.0,
                "confidence_std": np.std(list(self.confidence_scores)) if self.confidence_scores else 0.0,
                "min_confidence": np.min(list(self.confidence_scores)) if self.confidence_scores else 0.0,
                "max_confidence": np.max(list(self.confidence_scores)) if self.confidence_scores else 0.0,
                "confidence_trend": self.confidence_history
            },
            "performance_metrics": {
                "loss_history": self.loss_history[-1000:],  # Last 1000 steps
                "epoch_metrics": dict(self.epoch_metrics),
                "validation_metrics": self.validation_metrics
            }
        }
        
        with open(save_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"Training report saved to {save_path}")
        
    def train(self):
        """Main training loop with enhanced monitoring."""
        logger.info("Starting enhanced training with monitoring...")
        training_start_time = time.time()
        
        for epoch in range(self.num_epochs):
            self.train_epoch(epoch)
            
        total_training_time = time.time() - training_start_time
        logger.info(f"Training completed in {total_training_time:.2f}s")
        logger.info(f"Total steps: {self.step_count:,}")
        logger.info(f"Total drift alerts: {len(self.drift_alerts)}")
        logger.info(f"Average confidence: {np.mean(list(self.confidence_scores)):.3f}")
        
        # Generate final reports and plots
        self.create_monitoring_plots()
        self.generate_training_report()
        
    def save_model(self, save_path: str):
        """Save the trained model with monitoring data."""
        logger.info(f"Saving enhanced model to {save_path}")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config,
            'step_count': self.step_count,
            'drift_detector_state': self.drift_detector.get_state(),
            'training_metrics': {
                'loss_history': self.loss_history,
                'epoch_metrics': dict(self.epoch_metrics),
                'validation_metrics': self.validation_metrics,
                'confidence_scores': list(self.confidence_scores),
                'drift_alerts': self.drift_alerts
            }
        }, save_path)
        logger.info("Enhanced model saved successfully")
    
    def train_with_monitoring(self, train_dataset: list, val_dataset: list, num_epochs: int = 5, batch_size: int = 16) -> Dict[str, Any]:
        """
        Train the model with enhanced monitoring using provided datasets.
        
        Args:
            train_dataset: Training dataset samples
            val_dataset: Validation dataset samples
            num_epochs: Number of training epochs
            batch_size: Batch size for training
            
        Returns:
            Training results and metrics
        """
        logger.info(f"Starting enhanced training with monitoring for {num_epochs} epochs")
        
        # Setup model if not already done
        if not hasattr(self, 'model') or self.model is None:
            self.setup_model()
        
        # Create data loaders from provided datasets
        from torch.utils.data import DataLoader
        
        # Define collate function for Crickformer entries
        def crickformer_collate_fn(batch):
            """Collate function for Crickformer dataset entries."""
            batch_inputs = {}
            batch_targets = {}
            
            # Extract all inputs and targets
            for key in batch[0]['inputs'].keys():
                if key == 'recent_ball_history':
                    # Stack ball history: [batch_size, 5, 128] 
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                elif key == 'categorical_features':
                    # Handle categorical features - they're already integer tensors
                    # The static encoder expects shape [batch_size, num_categorical_features]
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                elif key in ['gnn_embeddings']:
                    # Keep these as they are (2D tensors)
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                elif key in ['video_mask']:
                    # Stack 1D mask tensors
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
                else:
                    # Stack 1D tensors: current_ball_features, numeric_features, video_features, market_odds
                    batch_inputs[key] = torch.stack([item['inputs'][key] for item in batch])
            
            for key in batch[0]['targets'].keys():
                batch_targets[key] = torch.stack([item['targets'][key] for item in batch])
            
            return {
                "inputs": batch_inputs,
                "targets": batch_targets
            }

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            drop_last=True,
            collate_fn=crickformer_collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            drop_last=False,
            collate_fn=crickformer_collate_fn
        )
        
        # Store original loaders
        original_train_loader = getattr(self, 'train_loader', None)
        original_val_loader = getattr(self, 'val_loader', None)
        
        # Set new loaders
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # Update training configuration
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        
        # Execute training
        try:
            self.train()
            
            # Generate training summary
            training_results = {
                "status": "completed",
                "epochs": num_epochs,
                "batch_size": batch_size,
                "train_samples": len(train_dataset),
                "val_samples": len(val_dataset),
                "device": str(self.device),
                "drift_detection_enabled": self.use_drift_detection,
                "confidence_estimation_enabled": self.use_confidence_estimation
            }
            
            return training_results
            
        finally:
            # Restore original loaders
            if original_train_loader is not None:
                self.train_loader = original_train_loader
            if original_val_loader is not None:
                self.val_loader = original_val_loader


def main():
    """Main function to parse arguments and launch enhanced training."""
    parser = argparse.ArgumentParser(description="Enhanced training with monitoring for Crickformer model.")
    parser.add_argument(
        "--config", 
        type=str, 
        default="config/train_config.yaml",
        help="Path to the training config file"
    )
    parser.add_argument(
        "--data-path",
        type=str,
        default="/Users/shamusrae/Library/Mobile Documents/com~apple~CloudDocs/Cricket /Data",
        help="Path to the cricket data directory"
    )
    parser.add_argument(
        "--train-matches",
        type=str,
        help="Path to file containing training match IDs"
    )
    parser.add_argument(
        "--val-matches",
        type=str,
        help="Path to file containing validation match IDs"
    )
    parser.add_argument(
        "--save-model",
        type=str,
        default="checkpoints/enhanced_crickformer.pth",
        help="Path to save the trained model"
    )
    parser.add_argument(
        "--use-csv",
        action="store_true",
        default=True,
        help="Use CSV data adapter (default: True)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Device to use for training"
    )
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    # Create enhanced trainer
    trainer = EnhancedTrainer(config, device=args.device)
    
    # Setup components
    trainer.setup_model()
    trainer.setup_dataset(
        args.data_path,
        use_csv=args.use_csv,
        train_matches_path=args.train_matches,
        val_matches_path=args.val_matches
    )
    
    # Train the model
    trainer.train()
    
    # Save the model
    trainer.save_model(args.save_model)
    
    logger.info("âœ… Enhanced training completed successfully!")


if __name__ == "__main__":
    main() 